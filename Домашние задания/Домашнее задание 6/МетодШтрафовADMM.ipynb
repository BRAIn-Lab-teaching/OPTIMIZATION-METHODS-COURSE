{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsOVIBf1LCST"
   },
   "source": [
    "# Метод штрафов. ADMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SrRbuH-LCSX"
   },
   "source": [
    "## Вступление\n",
    "\n",
    "Ранее мы частенько работали с ERM на основе логистической регрессии, однако, в данной домашней работе к рассмотрению будет предложена функция, лежащая в основне SVM (чисто ради разнообразия). Ставится следующая задача оптимизации:\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min \\limits_{w \\in \\mathbb{R}^d} \\frac{1}{2} \\| w \\|^2 + C \\sum \\limits_{i = 1}^n \\max \\left(1 - y_i \\cdot \\langle X_i, w \\rangle , 0 \\right).\n",
    "\\end{equation}\n",
    "$$\n",
    "Член с суммой есть ни что иное, как *hinge_loss*, довольно популярная функция потерь при обучении классификаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-PTsVT8LCSY"
   },
   "source": [
    "## Основная часть (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ow20-38LCSY"
   },
   "source": [
    "__a) (1 балл)__ Переформулируйте задачу (1) в задачу оптимизации с ограничениями. *Подсказка*: попробуйте представить слагаемое в сумме как $s_i = (Bw)_i + e_i$, где $B \\in \\mathbb{R}^{n \\times d}$ -- некоторая матрица, а $e \\in \\mathbb{R}^n$ -- вектор, составленный из единиц. Для удобства обозначений также замените суммирование на функцию\n",
    "$$\n",
    "p(s) = \\sum \\limits_{i = 1}^n \\max(s_i, 0).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY_BhQiYLCSZ"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAPC9W71LCSa"
   },
   "source": [
    "__б) (1.5 балла)__ Для полученной задачи оптимизации с ограничениями напишите функцию Лагранжа $L(w, s, \\lambda)$. В качестве ограничений можно взять функцию $s$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yc9M83JILCSb"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-Casj6tLCSb"
   },
   "source": [
    "__в) (0.5 балл)__ Запишите расширенный Лагранжиан задачи $L_{\\rho}(w, s, \\lambda)$, введя параметр $\\rho$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPlnPEzlLCSb"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgnuXYqrLCSc"
   },
   "source": [
    "__г) (2 балла)__ Как вы помните из лекции, на шаге алгоритма ADMM мы решаем задачу минимизации расширенного Лагранжиана по параметру $s$:\n",
    "$$\n",
    "\\varphi (w) = \\min \\limits_{s} L_{\\rho} (w, s, \\lambda).\n",
    "$$\n",
    "Переформулируйте задача минимизации так, чтобы члены с $w$ и $\\lambda$ были вытащены из под $\\min$. То есть вид должен быть такой:\n",
    "$$\n",
    "\\varphi (w) = c_1 \\|w\\|^2 + c_2 \\| \\lambda\\|^2 + c_3 \\min \\limits_{s} \\left[\\|s - t(w)\\|^2 + c_4 p(s) \\right].\n",
    "$$\n",
    "Здесь $t(w)$ -- функция, которая зависит только от переменной $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNh77cfGLCSc"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVe9yWmjLCSc"
   },
   "source": [
    "__д) (2 балла)__ Вычислите градиент $\\nabla \\varphi (w)$ аналитически. Считайте, что в точке оптимума значение $s^* = s^*(w^*) = \\text{Prox}_p^{\\rho^{-1}} (t(w^*))$. Воспользовавшись знанием с предыдущих лекций, покажите, что \n",
    "$$\n",
    "\\left[ \\text{Prox}_p^{\\rho^{-1}}\\left(z \\right) \\right]_i = \\begin{cases} z_i - \\frac{C}{\\rho}, & z_i > \\frac{C}{\\rho},\\\\ 0, & 0 \\leq z_i \\leq \\frac{C}{\\rho}, \\\\ z_i, & z_i < 0. \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrkUcceuLCSd"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msIdLLwaLCSd"
   },
   "source": [
    "__е) (3 балла)__ Напомним основные итерации ADMM на $k$-ой итерации:\n",
    "1. Вычислить новые значения $w^{k + 1}$ и $s^{k + 1}$ при решении задачи минимизации $\\min \\varphi (w)$:\n",
    "$$w^* = \\arg\\min \\varphi (w); \\qquad s^* = s^*(w^*) = \\text{Prox}_p^{\\rho^{-1}} (t(w^*))$$\n",
    "2. Обновить параметр $\\lambda^{k + 1}$ по теории (_покажите это!_):\n",
    "   $$\\lambda^{k + 1} = \\lambda^{k} - \\rho (s - Bw - e).$$ \n",
    "\n",
    "Реализуйте метод ```ADMM``` для решения задачи оптимизации ```hinge_loss```, проверьте сходимости при разных значениях коэффициента $C$. Рассмотрите следующие значения: [1e-10, 1e-8, 1e-6]. Параметр $\\rho$ положите равным 1. Критерием останова будет являться норма градиента функции $\\phi$ в новой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NkDy4JPZLCSd"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown + Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"mushrooms.txt\" \n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "data = load_svmlight_file(dataset)\n",
    "X, y = data[0].toarray(), data[1]\n",
    "\n",
    "#(преобразуйте метки к формату [-1, +1])\n",
    "y = ...\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIkwaLniem9V"
   },
   "source": [
    "## Дополнительная часть (всего 10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__а) (3 балла)__ На лекции был использован метод первого порядка в качестве спуска по переменным. Однако, такой метод проигрывает по итеративной сложности методу Ньютона... Но функция-то в нашем случае не дважды диффиренцируема. Значит нужно рассматривать суб-гессиан. Найдите второй суб-дифференциал $\\partial ^2 \\varphi(w)$.\n",
    "\n",
    "(*Указание:* возьмите честно второй раз дифференциал от формулы из пункта __д)__, используя градиент в тех местах, когда это возможно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваше решение (Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVdvEEBQT94f"
   },
   "source": [
    "__б) (7 баллов)__ Запишем теперь [полугладкий метод Ньютона](https://www.polyu.edu.hk/ama/profile/dfsun/A%20QUADRATICALLY%20CONVERGENT%20NEWTON%20METHOD%20FOR_published.pdf) для поиска аргмина. Так как мы не умеем вычислять точно гессиан оптимизируемой функции, то прибегнем к методу сопряженных градиентов для подсчета направления спуска. На $k$-ой итерации:\n",
    "1. Выбрать элемент $V_k \\in \\partial^2 \\varphi(w^k)$.\n",
    "2. Выполнить процедуру сопряженного градиента для поиска аппроксимации решения $d^k$ уравнения\n",
    "$$\n",
    "V_k d^k + \\nabla \\varphi(w^k) = 0.\n",
    "$$\n",
    "3. При помощи линейного поиска, как в методе BFGS находим параметр $\\alpha^k$ и делаем обновление переменной:\n",
    "$$w^{k + 1} = w^k + \\alpha_k d^k.$$\n",
    "\n",
    "Теперь, мы наконец-то готовы все собрать в один метод ADMM. На $k$-ой итерации\n",
    "1. Вычислить новые значения $w^{k + 1}$ используя полугладкий метод Ньютона.\n",
    "2. Обновить $s^{k + 1} = \\text{Prox}_{p}^{\\rho^{-1}} (w^{k + 1})$.\n",
    "3. Обновить параметр $\\lambda^{k + 1}$ по теории.\n",
    "\n",
    "Реализуйте метод ```ADMM_SemiSmoothNewton``` и постройте график зависимости критерия от итерации. Поиграйте со значениями параметров, постройте сравнительные графики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m43WMsPndw8c"
   },
   "outputs": [],
   "source": [
    "# Ваше решение (Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BZoRB2MZ_3k"
   },
   "source": [
    "# Интересно почитать\n",
    "\n",
    "Казалось бы, метод ADMM выглядит более громоздко, нежели привычные методы оптимизации без ограничений (SGD, NAG, DNM и другие), однако, на практике у него довольно много применений. Если вас заинтересовала возможность узнать больше о практических применения, рекоммендуется к прочтению [статья](https://web.stanford.edu/~boyd/papers/pdf/admm_distr_stats.pdf#page=64.25) от одного из столпов оптимизации Бойда. В ней рассматривается distributed подход (с множеством вычислительных единиц) к использованию метода ADMM при решении выпуклых задач оптимизации в машинном обучении и не только.   "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
