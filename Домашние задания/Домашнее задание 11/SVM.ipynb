{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание 11, SVM.\n",
    "### Deadline -  29.11.2024    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основная часть\n",
    "\n",
    "Для работы домашнего задания, необходимо загрузить файл svm.py по тому же пути, по которому лежит этот файл.\n",
    "\n",
    "Эта домашняя работа будет отличаться от предыдущих и ее идея показать и рассказать как рабобоает SVM с разными методами нахождения оптимума. Поэтому в этой работе не будет заданий по написанию большой части кода или вывода двойстенных задач."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import make_classification, make_circles\n",
    "pd.options.display.max_columns = 200\n",
    "import svm\n",
    "from svm import SVM, visualize\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Введение в SVM\n",
    "\n",
    "Пусть перед нами стоит задачи банарной классификации. Будем считать, что $y_i \\in \\{ -1, 1\\}; \\quad y \\in \\mathbb{R}^d$.\n",
    "\n",
    "Запишем нашу первоначальную задачу в виде задачи нахождения разделяющей гиперплоскости:\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\omega x_i + b \\geqslant 1, \\quad y_i = 1 \\\\\n",
    "        \\omega x_i + b < 1, \\quad y_i = -1\n",
    "    \\end{cases}\n",
    "$$.\n",
    "Запишем задачу формально:\n",
    "$$\n",
    "    \\sum_{i} \\mathbb{I} \\left[ y_i \\not = sign( \\langle \\omega, x_i \\rangle + b) \\right] \\rightarrow \\min_{\\omega}\\\\\n",
    "    \\sum_{i} \\mathbb{I} \\left[ y_i ( \\langle \\omega, x_i \\rangle + b) < 0\\right] \\rightarrow \\min_{\\omega}\n",
    "$$\n",
    "\n",
    "Велечина $M = y_i ( \\langle \\omega, x_i \\rangle + b)$ называется отступом классификатора. Легко увидеть, что \n",
    "1) Отступ положителен, когда $sign(y_i) = sing(\\langle \\omega, x_i \\rangle + b)$, то есть класс угадан верно. При этом чем больше отступ, тем больше расстояние от $x_i$ до разделяющей гиперплоскости\n",
    "2) Отступ отрицателен, когда $sign(y_i) \\not = sing(\\langle \\omega, x_i \\rangle + b)$, то есть класс угадан неверно. При этом чем больше отступ, тем больше ошибается классификатор.\n",
    "\n",
    "\n",
    "В некоторых случаях гиперплоскость можно расположить несколькими способами. Поэтому возникает логичное желание не только найти разделяющую прямую, а еще и провести ее на одинаковом удалении от обоих классов, то есть максимизировать минимальный отступ.\n",
    "\n",
    "Это можно сделать, положив функцию ошибки равной $max(0, 1 - M)$. Добавим к задаче $L_2$ регуляризацию и получим следующую задачу:\n",
    "$$\n",
    "    \\frac{1}{2}||\\omega||^2 + C\\sum_{i} \\max (0, 1 - y_i ( \\langle \\omega, x_i \\rangle + b)) \\rightarrow \\min_{\\omega}\n",
    "$$\n",
    "\n",
    "Также легко к этой задаче можно найти двойственную, что Вы уже проделывали в домашнем задании 9. На всякий случай запишем и двойственную задачу:\n",
    "$$\n",
    "    -\\sum_{i=1}^d \\lambda_i + \\frac{1}{2}\\sum_{i=1}^{d}\\sum_{j=1}^{d} \\lambda_i\\lambda_j y_iy_j\\langle x_i, x_j \\rangle \\rightarrow \\min_{\\lambda}\\\\\n",
    "    s.t. \\quad 0 \\leqslant \\lambda_i \\leqslant C, \\quad i = 1,...,d;\\\\\n",
    "    \\sum_{i=1}^d \\lambda_iy_i = 0\n",
    "$$\n",
    "\n",
    "__Hint__: На самом деле эта задача двойственная к немного другой, если хотите строгого теоретического вывода, то посмотрите [презентацию К.В.Воронцова](http://machinelearning.ru/wiki/images/archive/a/a0/20150316112120!Voron-ML-Lin-SVM.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача 1. (всего 2.5 балла)__ Сравнение различных способов нахождения решения линейных задачи.\n",
    "\n",
    "В библиотеке реализованы следующие методы решения задачи SVM:\n",
    "1) Метод внутренней точки для решения прямой задачи\n",
    "2) Метод внутренней точки для решения двойственной задачи\n",
    "3) Метод субградиентного спуска для решения прямой задачи\n",
    "4) Метод стохастического субградиентного спуска для решения прямой задачи\n",
    "5) Метод liblinear из библиотеки liblinear (решает задачи оптимизации путем последовательного выполнения приближенной минимизации вдоль координатных направлений или координатных гиперплоскостей)\n",
    "6) Метод libsvm из библиотеки libsvm (решает задачу оптимизации путем разбиения задачи на ряд наименьших возможных подзадач, которые затем решаются аналитически)\n",
    "\n",
    "__а). (1 балл)__ Сравнение временной сложности способов в зависимости от размерности. \n",
    "\n",
    "В этой задаче вам предстоит выбрать размерности, на которых будут сравниваться методы, дописать недостающих код и вывести на график зависимость времени работы от размерности задачи.\n",
    "\n",
    "Рекомендуем выбрать как минимум 10 различных размерностей из отрезка [50, 1000]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация различных методов, эти строки не нужно менять\n",
    "Pclf = SVM(method='primal')\n",
    "Dclf = SVM(method='dual', kernel='linear')\n",
    "Sclf = SVM(method='subgradient')\n",
    "StochSclf = SVM(method='stoch_subgradient')\n",
    "LLclf = SVM(method='liblinear')\n",
    "LSclf = SVM(method='libsvm', kernel='linear')\n",
    "rng = np.random.default_rng(4202)\n",
    "\n",
    "def generate_dataset(n_samples: int, dim: int, rnd = 4202):\n",
    "    return make_classification(n_samples=n_samples, n_features=int(dim*1.2), n_informative=dim, random_state=rnd)\n",
    "\n",
    "N = 300 # рекомендуемое количество n_samples в генерации датасета\n",
    "D_array = 0 # размерности задач\n",
    "\n",
    "# добавьте сохранение времени рана для каждого из методов\n",
    "for D in D_array:\n",
    "    X, y = generate_dataset(N, D)\n",
    "    y = y[:, np.newaxis]\n",
    "\n",
    "    # ниже код запуска самих алгоритмов, сами строки запуска не нужно менять,\n",
    "    # но Вы можете дописывать нужный код между этих строк\n",
    "    Pclf.fit(X,y)\n",
    "    \n",
    "    Dclf.fit(X,y)\n",
    "\n",
    "    Sclf.fit(X,y)\n",
    "    \n",
    "    StochSclf.fit(X,y)\n",
    "    \n",
    "    LLclf.fit(X,y)\n",
    "    \n",
    "    LSclf.fit(X,y)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# постройте график зависимости времени от размерности, добавьте на него все методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваше решение__ \n",
    "\n",
    "Опишите наблюдаемые на графиках результаты. Как думаете, почему они такие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__б). (1.5 балла)__ Сравнение временной сложности способов в зависимости от количества элементов. \n",
    "\n",
    "В этой задаче вам предстоит выбрать количество элементов в датасете, на котором будут сравниваться методы, и вывести на график зависимость времени работы от количества элементов в датасете.\n",
    "\n",
    "Рекомендуем выбрать как минимум 10 различных значений из отрезка [300, 1500]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация различных методов, эти строки не нужно менять\n",
    "Pclf = SVM(method='primal')\n",
    "Dclf = SVM(method='dual', kernel='linear')\n",
    "Sclf = SVM(method='subgradient')\n",
    "StochSclf = SVM(method='stoch_subgradient')\n",
    "LLclf = SVM(method='liblinear')\n",
    "LSclf = SVM(method='libsvm', kernel='linear')\n",
    "\n",
    "\n",
    "D = 200 # рекомендуемая размерность задачи\n",
    "N_array = 0 # количество элементов в датасете\n",
    "\n",
    "# добавьте сохранение времени рана для каждого из методов\n",
    "for N in N_array:\n",
    "    X, y = generate_dataset(N, D, 4202)\n",
    "    y = y[:, np.newaxis]\n",
    "\n",
    "    Pclf.fit(X,y)\n",
    "    \n",
    "    Dclf.fit(X,y)\n",
    "\n",
    "    Sclf.fit(X,y)\n",
    "    \n",
    "    StochSclf.fit(X,y)\n",
    "    \n",
    "    LLclf.fit(X,y)\n",
    "    \n",
    "    LSclf.fit(X,y)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# постройте график зависимости времени от количества элементов в датасете, добавьте на него все методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваше решение__ \n",
    "\n",
    "Опишите наблюдаемые на графиках результаты. Как думаете, почему они такие?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в решение нелинейных задач классификации.\n",
    "Порой бывает так, что данное нам множество не может быть линейно отделимо, но нам все еще хочется использовать SVM для решения задачи. Как в таком случае быть? \n",
    "\n",
    "Нужно найти способ из нелинейного множества сделать линейное! В этом нам могут помочь преобразование в другое пространство (возможно с большей размерностью) с помощью ядер. Это помогает из изначальных данных сделать линейно-отделимые.\n",
    "\n",
    "Приведем примеры нескольких ядер:\n",
    "1) линейное - $K(x, y) = x^T y$\n",
    "2) полиномиальное - $K(x, y) = (x^Ty + r)^p$, $\\quad r \\geqslant 0, q \\geqslant 1$\n",
    "3) радиальное базисное (RBF) - $K(x, y) = \\exp(-\\lambda ||x - y||^2)$, $\\quad \\lambda > 0$\n",
    "4) сигмоидное $K(x, y) = \\tanh (\\beta_0 x^Ty + \\beta_1)$\n",
    "\n",
    "В этом случае двойственная задача решается следующим образом:\n",
    "\n",
    "$$\n",
    "    -\\sum_{i=1}^d \\lambda_i + \\frac{1}{2}\\sum_{i=1}^{d}\\sum_{j=1}^{d} \\lambda_i\\lambda_j y_iy_j K(x_i, x_j) \\rightarrow \\min_{\\lambda}\\\\\n",
    "    s.t. \\quad 0 \\leqslant \\lambda_i \\leqslant C, \\quad i = 1,...,d;\\\\\n",
    "    \\sum_{i=1}^d \\lambda_iy_i = 0\n",
    "$$\n",
    "\n",
    "__Задача 2. (всего 2.5 балла)__ Сравнение различных способов нахождения решения нелинейных задачи.\n",
    "\n",
    "В библиотеке реализованы следующие методы решения нелинейной задачи с использованием RBF ядра.\n",
    "1) Метод внутренней точки для решения двойственной задачи\n",
    "2) Метод libsvm из библиотеки libsvm (решает задачу оптимизации путем разбиения задачи на ряд наименьших возможных подзадач, которые затем решаются аналитически)\n",
    "\n",
    "__Hint__ Т.к. задача генерации $n$-мерных шаров трудна, то будет сравнивать работу на линейной задачи. Это все равно не отражается на результатах.\n",
    "\n",
    "__а). (1 балл)__ Сравнение временной сложности алгоритмов с RBF ядром в зависимости от размерности. \n",
    "\n",
    "В этой задаче вам предстоит выбрать размерности, на которых будут сравниваться методы, и вывести на график зависимость времени работы от размерности задачи.\n",
    "\n",
    "Рекомендуем выбрать как минимум 10 различных размерностей из отрезка [10, 2000]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализация различных методов, эти строки не нужно менять\n",
    "Dclf = SVM(method='dual', kernel='rbf', gamma=0.1)\n",
    "LSclf = SVM(method='libsvm', kernel='rbf', gamma=0.1)\n",
    "\n",
    "\n",
    "N = 300 # рекомендуемое количество n_samples в генерации датасета\n",
    "D_array = 0 # размерность задачи\n",
    "\n",
    "# добавьте сохранение времени рана для каждого из методов\n",
    "for D in D_array:\n",
    "    X, y = generate_dataset(N, D)\n",
    "    y = y[:, np.newaxis]\n",
    "    \n",
    "    # ниже код запуска самих алгоритмов, сами строки запуска не нужно менять,\n",
    "    # но Вы можете дописывать нужный код между этих строк\n",
    "    Dclf.fit(X, y)\n",
    "    \n",
    "    LSclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# постройте график зависимости времени от размерности, добавьте на него все методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваше решение__ \n",
    "\n",
    "Опишите наблюдаемые на графиках результаты. Как думаете, почему они такие?\n",
    "\n",
    "Также сравните результаты с решением линейной задачи без использования ядра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__б). (1.5 балла)__ Сравнение временной сложности решения задач c RBF ядром в зависимости от количества элементов. \n",
    "\n",
    "В этой задаче вам предстоит выбрать количество элементов в датасете, на котором будут сравниваться методы, и вывести на график зависимость времени работы от количества элементов в датасете.\n",
    "\n",
    "Рекомендуем выбрать как минимум 10 различных значений из отрезка [300, 1500]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dclf = SVM(method='dual', kernel='rbf', gamma=0.1)\n",
    "LSclf = SVM(method='libsvm', kernel='rbf', gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "D = 200 # рекомендуемая размерность задачи\n",
    "N_array = 0 # количество элементов в датасете\n",
    "\n",
    "# добавьте сохранение времени рана для каждого из методов\n",
    "for N in N_array:\n",
    "    X, y = generate_dataset(N, D)\n",
    "    y = y[:, np.newaxis]\n",
    "    \n",
    "    Dclf.fit(X, y)\n",
    "    \n",
    "    LSclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# постройте график зависимости времени от количества элементов в датасете, добавьте на него все методы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваше решение__ \n",
    "\n",
    "Опишите наблюдаемые на графиках результаты. Как думаете, почему они такие?\n",
    "\n",
    "Также сравните результаты с решением линейной задачи без использования ядра."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная часть\n",
    "\n",
    "Рассмотрим двойстеннцю задачу с RBF ядром подробнее, запишем ее:\n",
    "\n",
    "$$\n",
    "    -\\sum_{i=1}^d \\lambda_i + \\frac{1}{2}\\sum_{i=1}^{d}\\sum_{j=1}^{d} \\lambda_i\\lambda_j y_iy_j \\exp(-\\lambda ||x - y||^2) \\rightarrow \\min_{\\lambda}\\\\\n",
    "    s.t. \\quad 0 \\leqslant \\lambda_i \\leqslant C, \\quad i = 1,...,d;\\\\\n",
    "    \\sum_{i=1}^d \\lambda_iy_i = 0\n",
    "$$\n",
    "\n",
    "Как можно видеть, есть два гиперпараметра - $C, \\lambda$. В этой части задания мы подберем лучшие значения этих гиперпараметров для двух задач.\n",
    "\n",
    "__Задача 1. (всего 5 баллов)__ Подбор гиперпараметров для хорошо и плохо разделимой выборки. Сравнение результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Хорошо разделимая выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация датасета, раскомментируйте код\n",
    "'''\n",
    "N = 800\n",
    "D = 2\n",
    "batch = N // 4\n",
    "dataset = np.zeros((N, D))\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dataset[:, 0][:batch] = np.random.normal(loc=3, scale=1.2, size=batch)\n",
    "dataset[:, 1][:batch] = np.random.normal(loc=3, scale=1.2, size=batch)\n",
    "dataset[:, 0][batch: 2 * batch] = np.random.normal(loc=-3, scale=1.2, size=batch)\n",
    "dataset[:, 1][batch: 2 * batch] = np.random.normal(loc=-3, scale=1.2, size=batch)\n",
    "dataset[:, 0][2 * batch: 3 * batch] = np.random.normal(loc=3, scale=1.2, size=batch)\n",
    "dataset[:, 1][2 * batch: 3 * batch] = np.random.normal(loc=-3, scale=1.2, size=batch)\n",
    "dataset[:, 0][3 * batch:] = np.random.normal(loc=-3, scale=1.2, size=batch)\n",
    "dataset[:, 1][3 * batch:] = np.random.normal(loc=3, scale=1.2, size=batch)\n",
    "target = -np.ones((N, 1), dtype=np.int32)\n",
    "target[2 * batch:, 0] = 1\n",
    "\n",
    "plt.close()\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=target, cmap=plt.cm.jet)\n",
    "plt.title('Model Dataset', fontsize=16)\n",
    "plt.xlabel('x1', fontsize=16)\n",
    "plt.ylabel('x2', fontsize=16)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__а). (1 балл)__ Допишите недостающий код. Функция должна сохранять лучшее занчение mean accuracy, а также гиперпараметры gamma и C для этого лучшего значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BruteSearchCV(X, y, method, C_list, gamma_list):\n",
    "    best_mean_accuracy = -1.0\n",
    "    best_gamma = -1.0\n",
    "    best_C = -1.0\n",
    "    results = []\n",
    "    cv = KFold(n_splits=7, shuffle=True, random_state=42)\n",
    "    for gamma in gamma_list:\n",
    "        for C in C_list:\n",
    "            clf = svm.SVM(C=C, method=method, kernel='rbf', gamma=gamma)\n",
    "            acc_list = []\n",
    "            for train_index, test_index in cv.split(X, y):\n",
    "                X_train, X_test = X[train_index, :], X[test_index, :]\n",
    "                y_train, y_test = y[train_index, :], y[test_index, :]\n",
    "                clf.fit(X_train, y_train)\n",
    "                acc_list.append(accuracy_score(y_true=y_test, y_pred=clf.predict(X_test, return_classes=True).T[0]))\n",
    "            mean_acc_score = np.array(acc_list).mean()\n",
    "            if mean_acc_score > best_mean_accuracy:\n",
    "                # Ваш код, после написания удалите pass\n",
    "            results.append({'mean_accuracy': , 'gamma': , 'C': })\n",
    "    return {'best_mean_accuracy': best_mean_accuracy, 'best_gamma': best_gamma, 'best_C': best_C}, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__б). (1 балл)__ Допишите недостающий код запуска поиска с методами 'dual' и 'libsvm'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ран ячейки может занять до 10 минут\n",
    "gamma_list = 10.0**np.arange(-5, 6)\n",
    "C_list = 10.0**np.arange(-5, 6)\n",
    "dual_best, dual_table = # BruteSearchCV для 'dual'\n",
    "libsvm_best, libsvm_table = # BruteSearchCV для 'libsvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVM(method='libsvm', C=libsvm_best['best_C'], kernel='rbf', gamma=libsvm_best['best_gamma'])\n",
    "clf.fit(dataset, target)\n",
    "visualize(dataset, target, clf, show_vectors=True, title='Model Dataset decision regions',\n",
    "          return_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_table = pd.DataFrame(dual_table)\n",
    "libsvm_table = pd.DataFrame(libsvm_table)\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(6.3, 5))\n",
    "xx2 = dual_table.gamma\n",
    "xx1 = dual_table.C\n",
    "accuracy_mesh = dual_table.mean_accuracy\n",
    "sc = plt.scatter(xx1, xx2, c=accuracy_mesh, cmap=plt.cm.jet, s=740, vmin=0.0, vmax=1.0, marker='s')\n",
    "plt.title('Dependence of accuracy from C and gamma', fontsize=12)\n",
    "plt.xlabel('C', fontsize=12)\n",
    "plt.ylabel('gamma', fontsize=12)\n",
    "plt.xlim([C_list.min(), C_list.max()])\n",
    "plt.ylim([gamma_list.min(), gamma_list.max()])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.colorbar(sc)\n",
    "print('Results for dual method:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Плохо разделимая выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация датасета, раскомментируйте код\n",
    "'''\n",
    "N = 800\n",
    "D = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "means = np.zeros(D)\n",
    "covs = 2 * np.diag(np.ones(D))\n",
    "dataset = np.random.multivariate_normal(means, covs, N)\n",
    "batch = N // 2\n",
    "target = np.random.choice([1, -1], size=N)[:, np.newaxis]\n",
    "plt.close()\n",
    "plt.scatter(dataset[:, 0], dataset[:, 1], c=target, cmap=plt.cm.jet)\n",
    "plt.title('Model Dataset', fontsize=16)\n",
    "plt.xlabel('x1', fontsize=16)\n",
    "plt.ylabel('x2', fontsize=16)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ран ячейки может занять до 10 минут\n",
    "gamma_list = 10.0**np.arange(-5, 6)\n",
    "C_list = 10.0**np.arange(-5, 6)\n",
    "dual_best, dual_table = BruteSearchCV(dataset, target, 'dual', C_list, gamma_list)\n",
    "libsvm_best, libsvm_table = BruteSearchCV(dataset, target, 'libsvm', C_list, gamma_list)\n",
    "table = {'dual': dual_best, 'libsvm': libsvm_best}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVM(method='libsvm', C=libsvm_best['best_C'], kernel='rbf', gamma=libsvm_best['best_gamma'])\n",
    "clf.fit(dataset, target)\n",
    "visualize(dataset, target, clf, show_vectors=True, title='Model Dataset decision regions',\n",
    "          return_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_table = pd.DataFrame(dual_table)\n",
    "libsvm_table = pd.DataFrame(libsvm_table)\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(6.3, 5))\n",
    "xx2 = dual_table.gamma\n",
    "xx1 = dual_table.C\n",
    "accuracy_mesh = dual_table.mean_accuracy\n",
    "sc = plt.scatter(xx1, xx2, c=accuracy_mesh, cmap=plt.cm.jet, s=740, vmin=0.0, vmax=1.0, marker='s')\n",
    "plt.title('Dependence of accuracy from C and gamma', fontsize=12)\n",
    "plt.xlabel('C', fontsize=12)\n",
    "plt.ylabel('gamma', fontsize=12)\n",
    "plt.xlim([C_list.min(), C_list.max()])\n",
    "plt.ylim([gamma_list.min(), gamma_list.max()])\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.colorbar(sc)\n",
    "print('Results for dual method:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__в). (3 балла)__ Посмотрите на зависмость точности от различных $C$ и $\\gamma$. Напишите почему так происходит, подкрепите свои рассуждения необходимыми выкладками.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ваше решение__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
